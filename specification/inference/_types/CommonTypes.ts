/*
 * Licensed to Elasticsearch B.V. under one or more contributor
 * license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright
 * ownership. Elasticsearch B.V. licenses this file to you under
 * the Apache License, Version 2.0 (the "License"); you may
 * not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

import { RateLimitSetting } from '@inference/_types/Services'
import { UserDefinedValue } from '@spec_utils/UserDefinedValue'
import { Id } from '@_types/common'
import { float, integer, long } from '@_types/Numeric'

export class RequestChatCompletion {
  /**
   * A list of objects representing the conversation.
   * Requests should generally only add new messages from the user (role `user`).
   * The other message roles (`assistant`, `system`, or `tool`) should generally only be copied from the response to a previous completion request, such that the messages array is built up throughout a conversation.
   */
  messages: Array<Message>
  /**
   * The ID of the model to use.
   */
  model?: string
  /**
   * The upper bound limit for the number of tokens that can be generated for a completion request.
   */
  max_completion_tokens?: long
  /**
   * A sequence of strings to control when the model should stop generating additional tokens.
   */
  stop?: Array<string>
  /**
   * The sampling temperature to use.
   */
  temperature?: float
  /**
   * Controls which tool is called by the model.
   */
  tool_choice?: CompletionToolType
  /**
   * A list of tools that the model can call.
   */
  tools?: Array<CompletionTool>
  /**
   * Nucleus sampling, an alternative to sampling with temperature.
   */
  top_p?: float
}

export class AdaptiveAllocations {
  /**
   * Turn on `adaptive_allocations`.
   * @server_default false
   */
  enabled?: boolean
  /**
   * The maximum number of allocations to scale to.
   * If set, it must be greater than or equal to `min_number_of_allocations`.
   */
  max_number_of_allocations?: integer
  /**
   * The minimum number of allocations to scale to.
   * If set, it must be greater than or equal to 0.
   * If not defined, the deployment scales to 0.
   */
  min_number_of_allocations?: integer
}

/**
 * @codegen_names string, object
 */
export type CompletionToolType = string | CompletionToolChoice

/**
 * An object style representation of a single portion of a conversation.
 */
export interface ContentObject {
  /**
   * The text content.
   */
  text: string
  /**
   * The type of content.
   */
  type: string
}

/**
 * The function that the model called.
 */
export interface ToolCallFunction {
  /**
   * The arguments to call the function with in JSON format.
   */
  arguments: string
  /**
   * The name of the function to call.
   */
  name: string
}

/**
 * A tool call generated by the model.
 */
export interface ToolCall {
  /**
   * The identifier of the tool call.
   */
  id: Id
  /**
   * The function that the model called.
   */
  function: ToolCallFunction
  /**
   * The type of the tool call.
   */
  type: string
}

/**
 * @codegen_names string, object
 */
export type MessageContent = string | Array<ContentObject>

/**
 * An object representing part of the conversation.
 */
export interface Message {
  /**
   * The content of the message.
   */
  content?: MessageContent
  /**
   * The role of the message author.
   */
  role: string
  /**
   * The tool call that this message is responding to.
   */
  tool_call_id?: Id
  /**
   * The tool calls generated by the model.
   */
  tool_calls?: Array<ToolCall>
}

/**
 * The tool choice function.
 *
 */
export interface CompletionToolChoiceFunction {
  /**
   * The name of the function to call.
   */
  name: string
}

/**
 * Controls which tool is called by the model.
 */
export interface CompletionToolChoice {
  /**
   * The type of the tool.
   */
  type: string
  /**
   * The tool choice function.
   */
  function: CompletionToolChoiceFunction
}

/**
 * The completion tool function definition.
 */
export interface CompletionToolFunction {
  /**
   * A description of what the function does.
   * This is used by the model to choose when and how to call the function.
   */
  description?: string
  /**
   * The name of the function.
   */
  name: string
  /**
   * The parameters the functional accepts. This should be formatted as a JSON object.
   */
  parameters?: UserDefinedValue
  /**
   * Whether to enable schema adherence when generating the function call.
   */
  strict?: boolean
}

/**
 * A list of tools that the model can call.
 */
export interface CompletionTool {
  /**
   * The type of tool.
   */
  type: string
  /**
   * The function definition.
   */
  function: CompletionToolFunction
}

export class AlibabaCloudServiceSettings {
  /**
   * A valid API key for the AlibabaCloud AI Search API.
   */
  api_key: string
  /**
   * The name of the host address used for the inference task.
   * You can find the host address in the API keys section of the documentation.
   * @ext_doc_id alibabacloud-api-keys
   */
  host: string
  /**
   * This setting helps to minimize the number of rate limit errors returned from AlibabaCloud AI Search.
   * By default, the `alibabacloud-ai-search` service sets the number of requests allowed per minute to `1000`.
   */
  rate_limit?: RateLimitSetting
  /**
   * The name of the model service to use for the inference task.
   * The following service IDs are available for the `completion` task:
   *
   * * `ops-qwen-turbo`
   * * `qwen-turbo`
   * * `qwen-plus`
   * * `qwen-max รท qwen-max-longcontext`
   *
   * The following service ID is available for the `rerank` task:
   *
   * * `ops-bge-reranker-larger`
   *
   * The following service ID is available for the `sparse_embedding` task:
   *
   * * `ops-text-sparse-embedding-001`
   *
   * The following service IDs are available for the `text_embedding` task:
   *
   * `ops-text-embedding-001`
   * `ops-text-embedding-zh-001`
   * `ops-text-embedding-en-001`
   * `ops-text-embedding-002`
   */
  service_id: string
  /**
   * The name of the workspace used for the inference task.
   */
  workspace: string
}

export class AlibabaCloudTaskSettings {
  /**
   * For a `sparse_embedding` or `text_embedding` task, specify the type of input passed to the model.
   * Valid values are:
   *
   * * `ingest` for storing document embeddings in a vector database.
   * * `search` for storing embeddings of search queries run against a vector database to find relevant documents.
   */
  input_type?: string
  /**
   * For a `sparse_embedding` task, it affects whether the token name will be returned in the response.
   * It defaults to `false`, which means only the token ID will be returned in the response.
   */
  return_token?: boolean
}

export enum AlibabaCloudTaskType {
  completion,
  rerank,
  space_embedding,
  text_embedding
}

export enum AlibabaCloudServiceType {
  'alibabacloud-ai-search'
}

export class AmazonBedrockServiceSettings {
  /**
   * A valid AWS access key that has permissions to use Amazon Bedrock and access to models for inference requests.
   */
  access_key: string
  /**
   * The base model ID or an ARN to a custom model based on a foundational model.
   * The base model IDs can be found in the Amazon Bedrock documentation.
   * Note that the model ID must be available for the provider chosen and your IAM user must have access to the model.
   * @ext_doc_id amazonbedrock-models
   */
  model: string
  /**
   * The model provider for your deployment.
   * Note that some providers may support only certain task types.
   * Supported providers include:
   *
   * * `amazontitan` - available for `text_embedding` and `completion` task types
   * * `anthropic` - available for `completion` task type only
   * * `ai21labs` - available for `completion` task type only
   * * `cohere` - available for `text_embedding` and `completion` task types
   * * `meta` - available for `completion` task type only
   * * `mistral` - available for `completion` task type only
   */
  provider?: string
  /**
   * The region that your model or ARN is deployed in.
   * The list of available regions per model can be found in the Amazon Bedrock documentation.
   * @ext_doc_id amazonbedrock-models
   */
  region: string
  /**
   * This setting helps to minimize the number of rate limit errors returned from Watsonx.
   * By default, the `watsonxai` service sets the number of requests allowed per minute to 120.
   */
  rate_limit?: RateLimitSetting
  /**
   * A valid AWS secret key that is paired with the `access_key`.
   * For informationg about creating and managing access and secret keys, refer to the AWS documentation.
   * @ext_doc_id amazonbedrock-secret-keys
   */
  secret_key: string
}

export class AmazonBedrockTaskSettings {
  /**
   * For a `completion` task, it sets the maximum number for the output tokens to be generated.
   * @server_default 64
   */
  max_new_tokens?: integer
  /**
   * For a `completion` task, it is a number between 0.0 and 1.0 that controls the apparent creativity of the results.
   * At temperature 0.0 the model is most deterministic, at temperature 1.0 most random.
   * It should not be used if `top_p` or `top_k` is specified.
   */
  temperature?: float
  /**
   * For a `completion` task, it limits samples to the top-K most likely words, balancing coherence and variability.
   * It is only available for anthropic, cohere, and mistral providers.
   * It is an alternative to `temperature`; it should not be used if `temperature` is specified.
   */
  top_k?: float
  /**
   * For a `completion` task, it is a number in the range of 0.0 to 1.0, to eliminate low-probability tokens.
   * Top-p uses nucleus sampling to select top tokens whose sum of likelihoods does not exceed a certain value, ensuring both variety and coherence.
   * It is an alternative to `temperature`; it should not be used if `temperature` is specified.
   */
  top_p?: float
}

export enum AmazonBedrockTaskType {
  completion,
  text_embedding
}

export enum AmazonBedrockServiceType {
  amazonbedrock
}

export class AnthropicServiceSettings {
  /**
   * A valid API key for the Anthropic API.
   */
  api_key: string
  /**
   * The name of the model to use for the inference task.
   * Refer to the Anthropic documentation for the list of supported models.
   * @ext_doc_id anothropic-models
   */
  model_id: string
  /**
   * This setting helps to minimize the number of rate limit errors returned from Anthropic.
   * By default, the `anthropic` service sets the number of requests allowed per minute to 50.
   */
  rate_limit?: RateLimitSetting
}

export class AnthropicTaskSettings {
  /**
   * For a `completion` task, it is the maximum number of tokens to generate before stopping.
   */
  max_tokens: integer
  /**
   * For a `completion` task, it is the amount of randomness injected into the response.
   * For more details about the supported range, refer to Anthropic documentation.
   * @ext_doc_id anthropic-messages
   */
  temperature?: float
  /**
   * For a `completion` task, it specifies to only sample from the top K options for each subsequent token.
   * It is recommended for advanced use cases only.
   * You usually only need to use `temperature`.
   */
  top_k?: integer
  /**
   * For a `completion` task, it specifies to use Anthropic's nucleus sampling.
   * In nucleus sampling, Anthropic computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches the specified probability.
   * You should either alter `temperature` or `top_p`, but not both.
   * It is recommended for advanced use cases only.
   * You usually only need to use `temperature`.
   */
  top_p?: float
}

export enum AnthropicTaskType {
  completion
}

export enum AnthropicServiceType {
  anthropic
}

export class AzureAiStudioServiceSettings {
  /**
   * A valid API key of your Azure AI Studio model deployment.
   * This key can be found on the overview page for your deployment in the management section of your Azure AI Studio account.
   *
   * IMPORTANT: You need to provide the API key only once, during the inference model creation.
   * The get inference endpoint API does not retrieve your API key.
   * After creating the inference model, you cannot change the associated API key.
   * If you want to use a different API key, delete the inference model and recreate it with the same name and the updated API key.
   * @ext_doc_id azureaistudio-api-keys
   */
  api_key: string
  /**
   * The type of endpoint that is available for deployment through Azure AI Studio: `token` or `realtime`.
   * The `token` endpoint type is for "pay as you go" endpoints that are billed per token.
   * The `realtime` endpoint type is for "real-time" endpoints that are billed per hour of usage.
   * @ext_doc_id azureaistudio-endpoint-types
   */
  endpoint_type: string
  /**
   * The target URL of your Azure AI Studio model deployment.
   * This can be found on the overview page for your deployment in the management section of your Azure AI Studio account.
   */
  target: string
  /**
   * The model provider for your deployment.
   * Note that some providers may support only certain task types.
   * Supported providers include:
   *
   * * `cohere` - available for `text_embedding` and `completion` task types
   * * `databricks` - available for `completion` task type only
   * * `meta` - available for `completion` task type only
   * * `microsoft_phi` - available for `completion` task type only
   * * `mistral` - available for `completion` task type only
   * * `openai` - available for `text_embedding` and `completion` task types
   */
  provider: string
  /**
   * This setting helps to minimize the number of rate limit errors returned from Azure AI Studio.
   * By default, the `azureaistudio` service sets the number of requests allowed per minute to 240.
   */
  rate_limit?: RateLimitSetting
}

export class AzureAiStudioTaskSettings {
  /**
   * For a `completion` task, instruct the inference process to perform sampling.
   * It has no effect unless `temperature` or `top_p` is specified.
   */
  do_sample?: float
  /**
   * For a `completion` task, provide a hint for the maximum number of output tokens to be generated.
   * @server_default 64
   */
  max_new_tokens?: integer
  /**
   * For a `completion` task, control the apparent creativity of generated completions with a sampling temperature.
   * It must be a number in the range of 0.0 to 2.0.
   * It should not be used if `top_p` is specified.
   */
  temperature?: float
  /**
   * For a `completion` task, make the model consider the results of the tokens with nucleus sampling probability.
   * It is an alternative value to `temperature` and must be a number in the range of 0.0 to 2.0.
   * It should not be used if `temperature` is specified.
   */
  top_p?: float
  /**
   * For a `text_embedding` task, specify the user issuing the request.
   * This information can be used for abuse detection.
   */
  user?: string
}

export enum AzureAiStudioTaskType {
  completion,
  text_embedding
}

export enum AzureAiStudioServiceType {
  azureaistudio
}

export class AzureOpenAIServiceSettings {
  /**
   * A valid API key for your Azure OpenAI account.
   * You must specify either `api_key` or `entra_id`.
   * If you do not provide either or you provide both, you will receive an error when you try to create your model.
   *
   * IMPORTANT: You need to provide the API key only once, during the inference model creation.
   * The get inference endpoint API does not retrieve your API key.
   * After creating the inference model, you cannot change the associated API key.
   * If you want to use a different API key, delete the inference model and recreate it with the same name and the updated API key.
   * @ext_doc_id azureopenai-auth
   */
  api_key?: string
  /**
   * The Azure API version ID to use.
   * It is recommended to use the latest supported non-preview version.
   */
  api_version: string
  /**
   * The deployment name of your deployed models.
   * Your Azure OpenAI deployments can be found though the Azure OpenAI Studio portal that is linked to your subscription.
   * @ext_doc_id azureopenai
   */
  deployment_id: string
  /**
   * A valid Microsoft Entra token.
   * You must specify either `api_key` or `entra_id`.
   * If you do not provide either or you provide both, you will receive an error when you try to create your model.
   * @ext_doc_id azureopenai-auth
   */
  entra_id?: string
  /**
   * This setting helps to minimize the number of rate limit errors returned from Azure.
   * The `azureopenai` service sets a default number of requests allowed per minute depending on the task type.
   * For `text_embedding`, it is set to `1440`.
   * For `completion`, it is set to `120`.
   * @ext_doc_id azureopenai-quota-limits
   */
  rate_limit?: RateLimitSetting
  /**
   * The name of your Azure OpenAI resource.
   * You can find this from the list of resources in the Azure Portal for your subscription.
   * @ext_doc_id azureopenai-portal
   */
  resource_name: string
}

export class AzureOpenAITaskSettings {
  /**
   * For a `completion` or `text_embedding` task, specify the user issuing the request.
   * This information can be used for abuse detection.
   */
  user?: string
}

export enum AzureOpenAITaskType {
  completion,
  text_embedding
}

export enum AzureOpenAIServiceType {
  azureopenai
}

export class CohereServiceSettings {
  /**
   * A valid API key for your Cohere account.
   * You can find or create your Cohere API keys on the Cohere API key settings page.
   *
   * IMPORTANT: You need to provide the API key only once, during the inference model creation.
   * The get inference endpoint API does not retrieve your API key.
   * After creating the inference model, you cannot change the associated API key.
   * If you want to use a different API key, delete the inference model and recreate it with the same name and the updated API key.
   * @ext_doc_id cohere-api-keys
   */
  api_key: string
  /**
   * For a `text_embedding` task, the types of embeddings you want to get back.
   * Use `byte` for signed int8 embeddings (this is a synonym of `int8`).
   * Use `float` for the default float embeddings.
   * Use `int8` for signed int8 embeddings.
   * @server_default float
   */
  embedding_type?: CohereEmbeddingType
  /**
   * For a `completion`, `rerank`, or `text_embedding` task, the name of the model to use for the inference task.
   *
   * * For the available `completion` models, refer to the [Cohere command docs](https://docs.cohere.com/docs/models#command).
   * * For the available `rerank` models, refer to the [Cohere rerank docs](https://docs.cohere.com/reference/rerank-1).
   * * For the available `text_embedding` models, refer to [Cohere embed docs](https://docs.cohere.com/reference/embed).
   *
   * The default value for a text embedding task is `embed-english-v2.0`.
   */
  model_id?: string
  /**
   * This setting helps to minimize the number of rate limit errors returned from Cohere.
   * By default, the `cohere` service sets the number of requests allowed per minute to 10000.
   */
  rate_limit?: RateLimitSetting
  /**
   * The similarity measure.
   * If the `embedding_type` is `float`, the default value is `dot_product`.
   * If the `embedding_type` is `int8` or `byte`, the default value is `cosine`.
   */
  similarity?: CohereSimilarityType
}

export enum CohereTaskType {
  completion,
  rerank,
  text_embedding
}

export enum CohereServiceType {
  cohere
}

export enum CohereEmbeddingType {
  byte,
  float,
  int8
}

export enum CohereInputType {
  classification,
  clustering,
  ingest,
  search
}

export enum CohereSimilarityType {
  cosine,
  dot_product,
  l2_norm
}

export enum CohereTruncateType {
  END,
  NONE,
  START
}

export class CohereTaskSettings {
  /**
   * For a `text_embedding` task, the type of input passed to the model.
   * Valid values are:
   *
   * * `classification`: Use it for embeddings passed through a text classifier.
   * * `clustering`: Use it for the embeddings run through a clustering algorithm.
   * * `ingest`: Use it for storing document embeddings in a vector database.
   * * `search`: Use it for storing embeddings of search queries run against a vector database to find relevant documents.
   *
   * IMPORTANT: The `input_type` field is required when using embedding models `v3` and higher.
   */
  input_type?: CohereInputType
  /**
   * For a `rerank` task, return doc text within the results.
   */
  return_documents?: boolean
  /**
   * For a `rerank` task, the number of most relevant documents to return.
   * It defaults to the number of the documents.
   * If this inference endpoint is used in a `text_similarity_reranker` retriever query and `top_n` is set, it must be greater than or equal to `rank_window_size` in the query.
   */
  top_n?: integer
  /**
   * For a `text_embedding` task, the method to handle inputs longer than the maximum token length.
   * Valid values are:
   *
   * * `END`: When the input exceeds the maximum input token length, the end of the input is discarded.
   * * `NONE`: When the input exceeds the maximum input token length, an error is returned.
   * * `START`: When the input exceeds the maximum input token length, the start of the input is discarded.
   */
  truncate?: CohereTruncateType
}

export class EisServiceSettings {
  /**
   * The name of the model to use for the inference task.
   */
  model_id: string
  /**
   * This setting helps to minimize the number of rate limit errors returned.
   * By default, the `elastic` service sets the number of requests allowed per minute to `240` in case of `chat_completion`.
   */
  rate_limit?: RateLimitSetting
}

export enum EisTaskType {
  chat_completion
}

export enum EisServiceType {
  elastic
}

export class ElasticsearchServiceSettings {
  /**
   * Adaptive allocations configuration details.
   * If `enabled` is true, the number of allocations of the model is set based on the current load the process gets.
   * When the load is high, a new model allocation is automatically created, respecting the value of `max_number_of_allocations` if it's set.
   * When the load is low, a model allocation is automatically removed, respecting the value of `min_number_of_allocations` if it's set.
   * If `enabled` is true, do not set the number of allocations manually.
   */
  adaptive_allocations?: AdaptiveAllocations
  /**
   * The deployment identifier for a trained model deployment.
   * When `deployment_id` is used the `model_id` is optional.
   */
  deployment_id?: string
  /**
   * The name of the model to use for the inference task.
   * It can be the ID of a built-in model (for example, `.multilingual-e5-small` for E5) or a text embedding model that was uploaded by using the Eland client.
   * @ext_doc_id eland-import
   */
  model_id: string
  /**
   * The total number of allocations that are assigned to the model across machine learning nodes.
   * Increasing this value generally increases the throughput.
   * If adaptive allocations are enabled, do not set this value because it's automatically set.
   */
  num_allocations?: integer
  /**
   * The number of threads used by each model allocation during inference.
   * This setting generally increases the speed per inference request.
   * The inference process is a compute-bound process; `threads_per_allocations` must not exceed the number of available allocated processors per node.
   * The value must be a power of 2.
   * The maximum value is 32.
   */
  num_threads: integer
}

export class ElasticsearchTaskSettings {
  /**
   * For a `rerank` task, return the document instead of only the index.
   * @server_default true
   */
  return_documents?: boolean
}

export enum ElasticsearchTaskType {
  rerank,
  sparse_embedding,
  text_embedding
}

export enum ElasticsearchServiceType {
  elasticsearch
}

export class ElserServiceSettings {
  /**
   * Adaptive allocations configuration details.
   * If `enabled` is true, the number of allocations of the model is set based on the current load the process gets.
   * When the load is high, a new model allocation is automatically created, respecting the value of `max_number_of_allocations` if it's set.
   * When the load is low, a model allocation is automatically removed, respecting the value of `min_number_of_allocations` if it's set.
   * If `enabled` is true, do not set the number of allocations manually.
   */
  adaptive_allocations?: AdaptiveAllocations
  /**
   * The total number of allocations this model is assigned across machine learning nodes.
   * Increasing this value generally increases the throughput.
   * If adaptive allocations is enabled, do not set this value because it's automatically set.
   */
  num_allocations: integer
  /**
   * The number of threads used by each model allocation during inference.
   * Increasing this value generally increases the speed per inference request.
   * The inference process is a compute-bound process; `threads_per_allocations` must not exceed the number of available allocated processors per node.
   * The value must be a power of 2.
   * The maximum value is 32.
   *
   * > info
   * > If you want to optimize your ELSER endpoint for ingest, set the number of threads to 1. If you want to optimize your ELSER endpoint for search, set the number of threads to greater than 1.
   */
  num_threads: integer
}

export enum ElserTaskType {
  sparse_embedding
}

export enum ElserServiceType {
  elser
}

export class GoogleAiStudioServiceSettings {
  /**
   * A valid API key of your Google Gemini account.
   */
  api_key: string
  /**
   * The name of the model to use for the inference task.
   * Refer to the Google documentation for the list of supported models.
   * @ext_doc_id googleaistudio-models
   */
  model_id: string
  /**
   * This setting helps to minimize the number of rate limit errors returned from Google AI Studio.
   * By default, the `googleaistudio` service sets the number of requests allowed per minute to 360.
   */
  rate_limit?: RateLimitSetting
}

export enum GoogleAiStudioTaskType {
  completion,
  text_embedding
}

export enum GoogleAiServiceType {
  googleaistudio
}

export class GoogleVertexAIServiceSettings {
  /**
   * The name of the location to use for the inference task.
   * Refer to the Google documentation for the list of supported locations.
   * @ext_doc_id googlevertexai-locations
   */
  location: string
  /**
   * The name of the model to use for the inference task.
   * Refer to the Google documentation for the list of supported models.
   * @ext_doc_id googlevertexai-models
   */
  model_id: string
  /**
   * The name of the project to use for the inference task.
   */
  project_id: string
  /**
   * This setting helps to minimize the number of rate limit errors returned from Google Vertex AI.
   * By default, the `googlevertexai` service sets the number of requests allowed per minute to 30.000.
   */
  rate_limit?: RateLimitSetting
  /**
   * A valid service account in JSON format for the Google Vertex AI API.
   */
  service_account_json: string
}

export class GoogleVertexAITaskSettings {
  /**
   * For a `text_embedding` task, truncate inputs longer than the maximum token length automatically.
   */
  auto_truncate?: boolean
  /**
   * For a `rerank` task, the number of the top N documents that should be returned.
   */
  top_n?: integer
}

export enum GoogleVertexAITaskType {
  rerank,
  text_embedding
}

export enum GoogleVertexAIServiceType {
  googlevertexai
}

export class HuggingFaceServiceSettings {
  /**
   * A valid access token for your HuggingFace account.
   * You can create or find your access tokens on the HuggingFace settings page.
   *
   * IMPORTANT: You need to provide the API key only once, during the inference model creation.
   * The get inference endpoint API does not retrieve your API key.
   * After creating the inference model, you cannot change the associated API key.
   * If you want to use a different API key, delete the inference model and recreate it with the same name and the updated API key.
   * @ext_doc_id huggingface-tokens
   */
  api_key: string
  /**
   * This setting helps to minimize the number of rate limit errors returned from Hugging Face.
   * By default, the `hugging_face` service sets the number of requests allowed per minute to 3000.
   */
  rate_limit?: RateLimitSetting
  /**
   * The URL endpoint to use for the requests.
   */
  url: string
}

export enum HuggingFaceTaskType {
  text_embedding
}

export enum HuggingFaceServiceType {
  hugging_face
}

export class JinaAIServiceSettings {
  /**
   * A valid API key of your JinaAI account.
   *
   * IMPORTANT: You need to provide the API key only once, during the inference model creation.
   * The get inference endpoint API does not retrieve your API key.
   * After creating the inference model, you cannot change the associated API key.
   * If you want to use a different API key, delete the inference model and recreate it with the same name and the updated API key.
   * @ext_doc_id jinaAi-embeddings
   */
  api_key: string
  /**
   * The name of the model to use for the inference task.
   * For a `rerank` task, it is required.
   * For a `text_embedding` task, it is optional.
   */
  model_id?: string
  /**
   * This setting helps to minimize the number of rate limit errors returned from JinaAI.
   * By default, the `jinaai` service sets the number of requests allowed per minute to 2000 for all task types.
   * @ext_doc_id jinaAi-rate-limit
   */
  rate_limit?: RateLimitSetting
  /**
   * For a `text_embedding` task, the similarity measure. One of cosine, dot_product, l2_norm.
   * The default values varies with the embedding type.
   * For example, a float embedding type uses a `dot_product` similarity measure by default.
   */
  similarity?: JinaAISimilarityType
}

export class JinaAITaskSettings {
  /**
   * For a `rerank` task, return the doc text within the results.
   */
  return_documents?: boolean
  /**
   * For a `text_embedding` task, the task passed to the model.
   * Valid values are:
   *
   * * `classification`: Use it for embeddings passed through a text classifier.
   * * `clustering`: Use it for the embeddings run through a clustering algorithm.
   * * `ingest`: Use it for storing document embeddings in a vector database.
   * * `search`: Use it for storing embeddings of search queries run against a vector database to find relevant documents.
   */
  task?: JinaAITextEmbeddingTask
  /**
   * For a `rerank` task, the number of most relevant documents to return.
   * It defaults to the number of the documents.
   * If this inference endpoint is used in a `text_similarity_reranker` retriever query and `top_n` is set, it must be greater than or equal to `rank_window_size` in the query.
   */
  top_n?: integer
}

export enum JinaAITaskType {
  rerank,
  text_embedding
}

export enum JinaAIServiceType {
  jinaai
}

export enum JinaAISimilarityType {
  cosine,
  dot_product,
  l2_norm
}

export enum JinaAITextEmbeddingTask {
  classification,
  clustering,
  ingest,
  search
}

export class MistralServiceSettings {
  /**
   * A valid API key of your Mistral account.
   * You can find your Mistral API keys or you can create a new one on the API Keys page.
   *
   * IMPORTANT: You need to provide the API key only once, during the inference model creation.
   * The get inference endpoint API does not retrieve your API key.
   * After creating the inference model, you cannot change the associated API key.
   * If you want to use a different API key, delete the inference model and recreate it with the same name and the updated API key.
   * @ext_doc_id mistral-api-keys
   */
  api_key: string
  /**
   * The maximum number of tokens per input before chunking occurs.
   */
  max_input_tokens?: integer
  /**
   * The name of the model to use for the inference task.
   * Refer to the Mistral models documentation for the list of available text embedding models.
   * @ext_doc_id mistral-api-models
   */
  model: string
  /**
   * This setting helps to minimize the number of rate limit errors returned from the Mistral API.
   * By default, the `mistral` service sets the number of requests allowed per minute to 240.
   */
  rate_limit?: RateLimitSetting
}

export enum MistralTaskType {
  text_embedding
}

export enum MistralServiceType {
  mistral
}

export class OpenAIServiceSettings {
  /**
   * A valid API key of your OpenAI account.
   * You can find your OpenAI API keys in your OpenAI account under the API keys section.
   *
   * IMPORTANT: You need to provide the API key only once, during the inference model creation.
   * The get inference endpoint API does not retrieve your API key.
   * After creating the inference model, you cannot change the associated API key.
   * If you want to use a different API key, delete the inference model and recreate it with the same name and the updated API key.
   * @ext_doc_id openai-api-keys
   */
  api_key: string
  /**
   * The number of dimensions the resulting output embeddings should have.
   * It is supported only in `text-embedding-3` and later models.
   * If it is not set, the OpenAI defined default for the model is used.
   */
  dimensions?: integer
  /**
   * The name of the model to use for the inference task.
   * Refer to the OpenAI documentation for the list of available text embedding models.
   * @ext_doc_id openai-models
   */
  model_id: string
  /**
   * The unique identifier for your organization.
   * You can find the Organization ID in your OpenAI account under *Settings > Organizations*.
   */
  organization_id?: string
  /**
   * This setting helps to minimize the number of rate limit errors returned from OpenAI.
   * The `openai` service sets a default number of requests allowed per minute depending on the task type.
   * For `text_embedding`, it is set to `3000`.
   * For `completion`, it is set to `500`.
   */
  rate_limit?: RateLimitSetting
  /**
   * The URL endpoint to use for the requests.
   * It can be changed for testing purposes.
   * @server_default https://api.openai.com/v1/embeddings.
   */
  url?: string
}

export class OpenAITaskSettings {
  /**
   * For a `completion` or `text_embedding` task, specify the user issuing the request.
   * This information can be used for abuse detection.
   */
  user?: string
}

export enum OpenAITaskType {
  chat_completion,
  completion,
  text_embedding
}

export enum OpenAIServiceType {
  openai
}

export class VoyageAIServiceSettings {
  /**
   * The number of dimensions for resulting output embeddings.
   * This setting maps to `output_dimension` in the VoyageAI documentation.
   * Only for the `text_embedding` task type.
   * @ext_doc_id voyageai-embeddings
   */
  dimensions?: integer
  /**
   * The name of the model to use for the inference task.
   * Refer to the VoyageAI documentation for the list of available text embedding and rerank models.
   * @ext_doc_id voyageai-embeddings
   * @ext_doc_id voyageai-rerank
   */
  model_id: string
  /**
   * This setting helps to minimize the number of rate limit errors returned from VoyageAI.
   * The `voyageai` service sets a default number of requests allowed per minute depending on the task type.
   * For both `text_embedding` and `rerank`, it is set to `2000`.
   */
  rate_limit?: RateLimitSetting
  /**
   * The data type for the embeddings to be returned.
   * This setting maps to `output_dtype` in the VoyageAI documentation.
   * Permitted values: float, int8, bit.
   * `int8` is a synonym of `byte` in the VoyageAI documentation.
   * `bit` is a synonym of `binary` in the VoyageAI documentation.
   * Only for the `text_embedding` task type.
   * @ext_doc_id voyageai-embeddings
   */
  embedding_type?: float
}

export class VoyageAITaskSettings {
  /**
   * Type of the input text.
   * Permitted values: `ingest` (maps to `document` in the VoyageAI documentation), `search` (maps to `query` in the VoyageAI documentation).
   * Only for the `text_embedding` task type.
   */
  input_type?: string
  /**
   * Whether to return the source documents in the response.
   * Only for the `rerank` task type.
   * @server_default false
   */
  return_documents?: boolean
  /**
   * The number of most relevant documents to return.
   * If not specified, the reranking results of all documents will be returned.
   * Only for the `rerank` task type.
   */
  top_k?: integer
  /**
   * Whether to truncate the input texts to fit within the context length.
   * @server_default true
   */
  truncation?: boolean
}

export enum VoyageAITaskType {
  text_embedding,
  rerank
}

export enum VoyageAIServiceType {
  voyageai
}

export class WatsonxServiceSettings {
  /**
   * A valid API key of your Watsonx account.
   * You can find your Watsonx API keys or you can create a new one on the API keys page.
   *
   * IMPORTANT: You need to provide the API key only once, during the inference model creation.
   * The get inference endpoint API does not retrieve your API key.
   * After creating the inference model, you cannot change the associated API key.
   * If you want to use a different API key, delete the inference model and recreate it with the same name and the updated API key.
   * @ext_doc_id watsonx-api-keys
   */
  api_key: string
  /**
   * A version parameter that takes a version date in the format of `YYYY-MM-DD`.
   * For the active version data parameters, refer to the Wastonx documentation.
   * @ext_doc_id watsonx-api-version
   */
  api_version: string
  /**
   * The name of the model to use for the inference task.
   * Refer to the IBM Embedding Models section in the Watsonx documentation for the list of available text embedding models.
   * @ext_doc_id watsonx-api-models
   */
  model_id: string
  /**
   * The identifier of the IBM Cloud project to use for the inference task.
   */
  project_id: string
  /**
   * This setting helps to minimize the number of rate limit errors returned from Watsonx.
   * By default, the `watsonxai` service sets the number of requests allowed per minute to 120.
   */
  rate_limit?: RateLimitSetting
  /**
   * The URL of the inference endpoint that you created on Watsonx.
   */
  url: string
}

export enum WatsonxTaskType {
  text_embedding
}

export enum WatsonxServiceType {
  watsonxai
}
