# summary:
description:
  Run `PUT _inference/chat-completion/llama-chat-completion` to create a Llama inference endpoint that performs a
  `chat_completion` task.
method_request: 'PUT _inference/chat-completion/llama-chat-completion'
# type: "request"
value: |-
  {
    "service": "llama",
    "service_settings": {
      "url": "http://localhost:8321/v1/openai/v1/chat/completions",
      "model_id": "llama3.2:3b" 
    }
  }
